defaults:
  - _self_
  - override hydra/launcher: submitit_slurm

compute:
  ngpus: 1
  nodes: 1

logging:
  log_freq: 100
  log_lr_every: ${logging.log_freq}
  log_file_name: stdout.log
  enable_wandb: True
  entity: mattias421
  project: flow_matching
  group: null

data:
  train: librispeech_dummy
  valid: librispeech_dummy
  cache_dir: /path/to/cache/dir
  num_workers: 8
  max_decode_ratio: 1
  debug: false

training:
  batch_size: 16
  snapshot: 500
  eval_freq: 500
  perplexity_freq: 50000000
  cer_freq: 500
  seed: 42
  unsupervised: false

eval:
  batch_size: 16
  sample_batch_size: 16
  perplexity: True
  perplexity_batch_size: 16
  n_gen_iter: 8
  kenlm_path: /path/to/kenlm

optim:
  weight_decay: 0.03
  optimizer: AdamW
  lr: 1e-3
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  warmup: 200
  grad_clip: 1.
  eta_min_ratio: 0.1
  fused: false
  n_iters: 40000
  log_lr_every: ${logging.log_lr_every}

flow:
  source_distribution: asr # [asr]
  loss_function: cross_entropy  # [cross_entropy, generalized_kl]
  exponent: 1.
  scheduler_type: polynomial
  sampling_steps: 64
  partial_noise_prob: 0.1

model:
  hidden_size: 768
  cond_dim: 128
  length: 1024
  n_blocks: 12
  n_heads: 12
  dropout: 0
  compile: true

hydra_dir: /path/to/hydra/dir

hydra:
  run:
    dir: ${hydra_dir}/${now:%Y.%m.%d}/${now:%H%M%S}
  sweep:
    dir: ${hydra_dir}/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}
  launcher:
    max_num_timeout: 100000
    timeout_min: 4320
    partition: gpu,gpu-h100,gpu-h100-nvl
    qos: gpu:1
    gpus_per_node: ${compute.ngpus}
    cpus_per_task: 8
    mem_gb: null
    mem_per_cpu: null
    mem_per_gpu: null
    srun_args:
        - --export=ALL
    setup:
        - ml binutils GCCcore GCC libsndfile cuDNN bzip2
        - source .venv/bin/activate
